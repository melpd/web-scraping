---
title: "Web-Scraping"
subtitle: "A Presentation by Melanie Desroches"
format:
    revealjs:
        self-contained: true
        slide-number: true
        preview-links: true
        theme: solarized
---

## What is Web-Scraping

- Web scraping is an automated process used to gather data from websites.
- Web scraping allows us to access and collect large amounts of data 
  directly from web pages if the information is not avalible for download.
- Websites are primarily structured with HTML (Hypertext Markup Language), 
  which organizes and displays content. Web scrapers parse through this 
  HTML code to identify and extract relevant information.
- Applications of web-scraping: sentiment analysis on social media, market 
  research, e-commerce

# How to Web-Scrape with Python

## Beautiful Soup

- The Beautiful Soup Python Library simplifies the process of parsing and 
  navigating HTML and XML documents, making it easier to extract data from 
  websites.
- Beautiful soup can be installed using 
```{bash}
pip install beautifulsoup4
```

- Beautiful Soup is ideal for scraping data from static websites. Static 
  websites do not change based on user actions or require server-side 
  interactions to update content dynamically.


## Selenium

- Selenium is used for web browser automation and dynamic websites
- Dynamic sites often use backend programming to pull data from a database, 
  customize it, and render it in real time based on user requests.
- Selenium can be installed using 
```{bash}
pip install selenium
```

- To control a web browser, Selenium requires a WebDriver. Download the driver 
  that matches your browser version and operating system


## Beautiful Soup vs Selenium
- Selenium is better for interacting with dynamic web content that loads JavaScript 
  or requires actions like clicking, scrolling, or filling forms
- Selenium can be slower and more resource-intensive since it opens a browser window 
  to simulate real user actions.
- Beautiful Soup is lightweight, easy to learn, and perfect for working with static 
  HTML content.
- Beautiful Soup is more limited when it comes to dynamic websites, which are much 
  more common nowadays

## A Step-by Step Guide to Web-Scraping
- Find the website URL with the information you want to select
- Use the requests package in Python to send an HTTP request to the URL
- Use the "Inspect" tool in your browser to identify the tags, classes, or IDs 
  associated with the data you want to extract.
- Use a parsing library like Beautiful Soup or Selenium to process the HTML response
- Clean and store the relevant infomation


# An Example Using NYC Crash Data

# Beautiful Soup and NYPD Precincts

## Finding the URL and sending the request
```{python}
#| echo: true
import requests
from bs4 import BeautifulSoup
import pandas as pd

# URL of the NYPD precincts page
url = "https://www.nyc.gov/site/nypd/bureaus/patrol/precincts-landing.page"

# Send a GET request to the page
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36"
}
response = requests.get(url, headers=headers)

# Check if the request was successful
print(response.status_code)
if response.status_code != 200:
    print(f"Failed to retrieve page: Status code {response.status_code}")
```

## Parsing the Table
```{python}
#| echo: true
# Parse the HTML content
soup = BeautifulSoup(response.text, 'html.parser')

# Find the table with class "rt" which holds the precinct data
table = soup.find("table", {"class": "rt"})
    
# Lists to hold the extracted data
precinct_names = []
addresses = []
    
# Extract each row of the table (each row corresponds to one precinct)
for row in table.find_all("tr"):
  # Find the "Precinct" and "Address" columns by data-label attribute
  precinct_cell = row.find("td", {"data-label": "Precinct"})
  address_cell = row.find("td", {"data-label": "Address"})
        
  # If both cells are found, store their text content
  if precinct_cell and address_cell:
    precinct_names.append(precinct_cell.get_text(strip=True))
    addresses.append(address_cell.get_text(strip=True))

# Create a DataFrame with the extracted data
precincts_df = pd.DataFrame({
  "Precinct": precinct_names,
  "Address": addresses
})

# Display the DataFrame
print(precincts_df)
```

# Selenium and Weather Data

## Set-Up Selenium and the WebDriver
```{python}
#| echo: true
import time
from selenium import webdriver
from selenium.webdriver.edge.service import Service
from selenium.webdriver.edge.options import Options

# Configure headless Edge browser
edge_options = Options()
edge_options.add_argument("--headless")
edge_options.add_argument("--disable-gpu")

# Set the path for Edge driver
edge_driver_path = 'C:/Users/mpdes/Downloads/edgedriver_win64/msedgedriver.exe'
service = Service(edge_driver_path)
driver = webdriver.Edge(service=service, options=edge_options)
```

## Go to the Page and Find the Target data
```{python}
#| echo: true
# Define the target URL
url = f"https://www.wunderground.com/history/weekly/us/ny/new-york-city/KLGA/date/2024-6-30"

# Load the page
driver.get(url)

from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

# Wait for table to load
wait = WebDriverWait(driver, 15)
table = wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "table.days")))
```

## Collect the data
```{python}
#| echo: true
# Initialize lists for each data type
dates = []
max_temps = []
min_temps = []
humidity_values = []
wind_speeds = []
pressure_values = []
precip_values = []

# Get all rows
rows = table.find_elements(By.CSS_SELECTOR, "tbody tr")
        
# Process the first row which contains all the dates
date_row = rows[0].text.split('\n')
dates = [date for date in date_row if date.isdigit()][:7]  # Get first 7 dates

# Find temperature values (rows 10-16 contain the actual temperature data)
temp_rows = rows[10:17]  # Get rows 10-16
for row in temp_rows:
  cells = row.find_elements(By.TAG_NAME, "td")
  if len(cells) >= 3:
    max_temps.append(cells[0].text.strip())
    min_temps.append(cells[2].text.strip())

# Find humidity values (rows 18-24)
humidity_rows = rows[18:25]
for row in humidity_rows:
  cells = row.find_elements(By.TAG_NAME, "td")
  if len(cells) >= 2:
    humidity_values.append(cells[1].text.strip())

# Find wind speed values (rows 26-32)
wind_rows = rows[26:33]
for row in wind_rows:
  cells = row.find_elements(By.TAG_NAME, "td")
  if len(cells) >= 1:
    wind_speeds.append(cells[0].text.strip())

# Find pressure values (rows 42-48)
pressure_rows = rows[42:49]
for row in pressure_rows:
  cells = row.find_elements(By.TAG_NAME, "td")
  if len(cells) >= 1:
    pressure_values.append(cells[0].text.strip())

# Find precipitation values (rows 50-56)
precip_rows = rows[50:57]
for row in precip_rows:
  cells = row.find_elements(By.TAG_NAME, "td")
  if len(cells) >= 1:
    precip_values.append(cells[0].text.strip())
```

## Store the Collected Data
```{python}
#| echo: true
import pandas as pd

# Create DataFrame
weather_data = pd.DataFrame({
    'Date': dates,
    'Max Temperature (°F)': max_temps,
    'Min Temperature (°F)': min_temps,
    'Humidity (%)': humidity_values,
    'Wind Speed (mph)': wind_speeds,
    'Pressure (in)': pressure_values,
    'Precipitation (in)': precip_values
})
print(weather_data)
```

# Data Ethics

## Why can Web-Scraping be un-ethical
- Just because you can web-scrape doesn’t always mean you should
- In order to be ethical data scientists, always be careful of where you are 
  getting the data from. Not all websites allow you to scrape data
- If you send too many requests at once, you can crash the website!

## Some Tips to Help You Scrape Ethically

- Never scrape from a website that requires login or payment
- Spread out the time of the requests in order to prevent the website from crashing
- Always be mindful of what kind of information you are trying to collect and if 
  it is private information/intellectual property
- Check a websites terms of servive to see if you are allowed to scrape


# Conclusion

## Summary

This presentation has covered:

- What Web Scraping is
- How to Web Scrape in Python
- How to Web Scrape Ethically


## Further Reading

- https://scrapfly.io/blog/web-scraping-with-selenium-and-python/
- https://www.browserstack.com/guide/web-scraping-using-selenium-python

For more information about Beautiful Soup:

- [Beautiful Soup Documentation](https://beautiful-soup-4.readthedocs.io/en/latest/)

For a useful guide on fundamental markdown syntax:

- [Markdown Basics -- Quarto](https://quarto.org/docs/authoring/markdown-basics.html)

# THANK YOU!