---
title: "Web-Scraping"
subtitle: "A Presentation by Melanie Desroches"
format:
    revealjs:
        slide-number: true
        preview-links: true
        theme: default
---

## What is Web-Scraping

- Web-Scraping is an automated technique used to collect information from websites
- Not all data can be easily downloaded from websites. Web scraping is used to extract large amounts of data from websites
  without having to copy and paste
- Applications of web-scraping: sentiment analysis on social media, market research

# How to Web-Scrape with Python

## Beautiful Soup

- Beautiful Soup was created in 2004 by Leonard Richardson
- The Beautiful Soup Python Library is used to web-scrape from HTML and XML files
- Beautiful soup can be installed using pip install beautifulsoup4


## Selenium

- Selenium was creates in 2004 by John Higgins
- Selenium can be installed using pip install selenium
- You also need to install a WebDriver (ChromeDriver, EdgeDriver, etc)


## Beautiful Soup vs Selenium
- Beautiful Soup works best with static websites (meaning that there is no server that updates the content and no database)
- Selenium works best with dynamic websites
- Beautiful Soup is easier to use and install
- Selenium tends to more compatible with more websites since more websites are dynamic

## A Step-by Step Guide
- Find the website URL with the information you want to select
- Send an HTTP request, using the requests package (generally 200-299 means the request has been granted and 
  400-499 means that your request is not allowed)
- Find the part of the website you want to extract using inspect
- Parse through the website's HTML content
- Clean and store the relevant infomation


# An Example Using Weather and NYC Crash Data

```{python}
#Beautiful Soup Version
import requests
from bs4 import BeautifulSoup
import pandas as pd

# Function to scrape weekly weather data from Wunderground
def scrape_weekly_weather_wunderground(date_str="2024-6-30"):
    # Construct the URL with the specified date
    url = f"https://www.wunderground.com/history/weekly/us/ny/new-york-city/KLGA/date/{date_str}"

    # Headers to simulate a browser request
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.88 Safari/537.36"
    }

    # Make a request to the URL
    response = requests.get(url, headers=headers)
    if response.status_code != 200:
        print(f"Failed to retrieve data: Status code {response.status_code}")
        return None

    # Parse the page
    soup = BeautifulSoup(response.text, 'html.parser')

    # Locate the observation table container
    table_container = soup.select_one('div.observation-table.ng-star-inserted')

    if not table_container:
        print("Observation table container not found.")
        return None

    # Locate the table inside the container
    table = table_container.select_one('table.days.ng-star-inserted')

    if not table:
        print("Weather data table not found within the container.")
        return None

    # Data containers
    dates = []
    max_temps = []
    min_temps = []
    humidity_avg = []
    wind_speed_max = []
    pressure_max = []
    precipitation = []

    # Extract rows from the table (each row represents a day)
    rows = table.find('tbody').find_all('tr')

    for row in rows:
        # Extract date
        date = row.find_all('td')[0].get_text(strip=True)
        dates.append(date)

        # Extract metrics (replace indices as needed based on table columns)
        max_temp = row.find_all('td')[1].get_text(strip=True)  # Max temperature
        min_temp = row.find_all('td')[2].get_text(strip=True)  # Min temperature
        humidity = row.find_all('td')[5].get_text(strip=True)  # Avg humidity
        wind_max = row.find_all('td')[7].get_text(strip=True)  # Max wind speed
        pressure = row.find_all('td')[9].get_text(strip=True)  # Max pressure
        precip = row.find_all('td')[11].get_text(strip=True)   # Total precipitation

        max_temps.append(max_temp)
        min_temps.append(min_temp)
        humidity_avg.append(humidity)
        wind_speed_max.append(wind_max)
        pressure_max.append(pressure)
        precipitation.append(precip)

    # Create a DataFrame with the data
    weather_df = pd.DataFrame({
        'Date': dates,
        'Max Temperature': max_temps,
        'Min Temperature': min_temps,
        'Humidity (Avg)': humidity_avg,
        'Wind Speed (Max)': wind_speed_max,
        'Pressure (Max)': pressure_max,
        'Precipitation': precipitation
    })

    return weather_df

# Specify the date to start the week (June 30, 2024)
weather_data = scrape_weekly_weather_wunderground(date_str="2024-6-30")

# Display the result if data is found
if weather_data is not None:
    print(weather_data)

```


```{python}
#Selenium Version
import time
from selenium import webdriver
from selenium.webdriver.edge.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.edge.options import Options
import pandas as pd
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

def scrape_weekly_weather_wunderground(date_str="2024-6-30"):
    url = f"https://www.wunderground.com/history/weekly/us/ny/new-york-city/KLGA/date/{date_str}"
    
    edge_options = Options()
    edge_options.add_argument("--headless")
    edge_options.add_argument("--disable-gpu")
    
    edge_driver_path = 'C:/Users/mpdes/Downloads/edgedriver_win64/msedgedriver.exe'
    
    service = Service(edge_driver_path)
    driver = webdriver.Edge(service=service, options=edge_options)
    
    try:
        driver.get(url)
        time.sleep(5)  # Wait for the page to load completely

        # Find the main data container
        table = driver.find_element(By.CSS_SELECTOR, "table.days")
        
        # Initialize lists for each data type
        dates = []
        max_temps = []
        min_temps = []
        humidity_values = []
        wind_speeds = []
        pressure_values = []
        precip_values = []

        # Get all rows
        rows = table.find_elements(By.CSS_SELECTOR, "tbody tr")
        
        # Process the first row which contains all the dates
        date_row = rows[0].text.split('\n')
        dates = [date for date in date_row if date.isdigit()][:7]  # Get first 7 dates

        # Find temperature values (rows 10-16 contain the actual temperature data)
        temp_rows = rows[10:17]  # Get rows 10-16
        for row in temp_rows:
            cells = row.find_elements(By.TAG_NAME, "td")
            if len(cells) >= 3:
                max_temps.append(cells[0].text.strip())
                min_temps.append(cells[2].text.strip())

        # Find humidity values (rows 18-24)
        humidity_rows = rows[18:25]
        for row in humidity_rows:
            cells = row.find_elements(By.TAG_NAME, "td")
            if len(cells) >= 2:
                humidity_values.append(cells[1].text.strip())

        # Find wind speed values (rows 26-32)
        wind_rows = rows[26:33]
        for row in wind_rows:
            cells = row.find_elements(By.TAG_NAME, "td")
            if len(cells) >= 1:
                wind_speeds.append(cells[0].text.strip())

        # Find pressure values (rows 42-48)
        pressure_rows = rows[42:49]
        for row in pressure_rows:
            cells = row.find_elements(By.TAG_NAME, "td")
            if len(cells) >= 1:
                pressure_values.append(cells[0].text.strip())

        # Find precipitation values (rows 50-56)
        precip_rows = rows[50:57]
        for row in precip_rows:
            cells = row.find_elements(By.TAG_NAME, "td")
            if len(cells) >= 1:
                precip_values.append(cells[0].text.strip())

        # Create DataFrame
        weather_data = pd.DataFrame({
            'Date': [f'{date}' for date in dates],
            'Max Temperature (°F)': max_temps,
            'Min Temperature (°F)': min_temps,
            'Humidity (%)': humidity_values,
            'Wind Speed (mph)': wind_speeds,
            'Pressure (in)': pressure_values,
            'Precipitation (in)': precip_values
        })

        return weather_data

    except Exception as e:
        print(f"An error occurred: {e}")
        return None
    finally:
        driver.quit()

# Run the scraper
weather_data = scrape_weekly_weather_wunderground(date_str="2024-6-30")

if weather_data is not None:
    print("\nWeather Data:")
    print(weather_data)
    
    # Optionally save to CSV
    weather_data.to_csv('weather_data.csv', index=False)

```

This one works!


# Data Ethics

## Why can Web-Scraping be un-ethical
- Just because you can web-scrape doesn’t always mean you should
- In order to be ethical data scientists, always be careful of where you are getting the data from. Not all websites allow you to scrape data
- If you send too many requests at once, you can crash the website!

## Some Tips to Help You Scrape Ethically

- Never scrape from a website that requires login or payment
- Spread out the time of the requests in order to prevent the website from crashing
- Always be mindful of what kind of information you are trying to collect and if it is private information/intellectual property


# Conclusion

## Summary

This presentation has covered:

- What Web Scraping is
- How to Web Scrape in Python
- How to Web Scrape Ethically


## Further Reading

For more information about Beautiful Soup:

- [Beautiful Soup Documentation](https://beautiful-soup-4.readthedocs.io/en/latest/)

For a useful guide on fundamental markdown syntax:

- [Markdown Basics -- Quarto](https://quarto.org/docs/authoring/markdown-basics.html)

# THANK YOU!!!